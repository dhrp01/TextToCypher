{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e534759b-3b65-46f5-b2ac-fc20a5cb03ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The scripts accelerate, accelerate-config, accelerate-estimate-memory, accelerate-launch and accelerate-merge-weights are installed in '/home/dhrumeenkish_umass_edu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch pandas tqdm accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2337c28e-5d6a-4cae-bd19-36c45a90c249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from TextToCypherDataLoader import Text2CypherDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Path to your dataset and models\n",
    "DATASET_PATH = \"/work/pi_wenlongzhao_umass_edu/9/dpatel/\"\n",
    "MODEL_PATH = \"/datasets/ai/\"\n",
    "DEEPSEEK_DISTILL_LLAMA_70B_PATH = \"deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-70B/snapshots/0d6d11a6ea1187363aa7b78543f824fc02e06b14\"\n",
    "DEEPSEEK_DISTILL_QWEN_7B_PATH = \"deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B/snapshots/6602cadec947dbb53e64f3d8d6425320b2197247\"\n",
    "\n",
    "DEEPSEEK_DISTILL_LLAMA_70B = os.path.join(MODEL_PATH, DEEPSEEK_DISTILL_LLAMA_70B_PATH)\n",
    "DEEPSEEK_DISTILL_QWEN_7B = os.path.join(MODEL_PATH, DEEPSEEK_DISTILL_QWEN_7B_PATH)\n",
    "\n",
    "dataset = load_dataset(\"neo4j/text2cypher-2024v1\")[\"train\"].shuffle(seed=42).select(range(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56fabf76-b58c-4da0-b54d-6a82f856e614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3674cf0fe2db41e3b53c47c5bb63658f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(DEEPSEEK_DISTILL_QWEN_7B)\n",
    "model = AutoModelForCausalLM.from_pretrained(DEEPSEEK_DISTILL_QWEN_7B, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1baa65f-bf72-4193-a1ef-aac75372dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "train_dataset = Text2CypherDataset(dataset, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d7d1ebf-5cbe-4203-abf3-4e8d1a8c3499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cypher(question, schema):\n",
    "    input_text = f\"Convert this natural language question to a Cypher query.\\nQuestion: {question}\\nSchema: {schema}\\nCypher Query:\"\n",
    "    \n",
    "    # inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(**inputs, max_length=512, max_new_tokens=256, do_sample=True, temperature=0.7)\n",
    "\n",
    "    generated_cypher = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_cypher.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcee445c-7a57-4661-8566-bb37ad263e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Cypher Queries:   0%|          | 0/2 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating Cypher Queries:  50%|█████     | 1/2 [22:39<22:39, 1359.23s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating Cypher Queries: 100%|██████████| 2/2 [27:41<00:00, 830.89s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /work/pi_wenlongzhao_umass_edu/9/dpatel/zero_shot_baseline_results.csv\n",
      "Accuracy: 0/10 (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch in tqdm(train_loader, desc=\"Generating Cypher Queries\"):\n",
    "    for i in range(len(batch[\"input_ids\"])):\n",
    "        question = dataset[i][\"question\"]\n",
    "        schema = dataset[i][\"schema\"]\n",
    "        true_cypher = dataset[i][\"cypher\"]\n",
    "\n",
    "        predicted_cypher = generate_cypher(question, schema)\n",
    "\n",
    "        is_correct = predicted_cypher.strip().lower() == true_cypher.strip().lower()\n",
    "        if is_correct:\n",
    "            correct_predictions += 1\n",
    "\n",
    "        total_samples += 1\n",
    "\n",
    "results.append({\n",
    "    \"Model Name\": DEEPSEEK_DISTILL_LLAMA_70B.split(\"/\")[-1],\n",
    "    \"Correctly Predicted\": correct_predictions,\n",
    "    \"Total Samples\": total_samples,\n",
    "    \"Accuracy\": f\"{(correct_predictions/total_samples)*100:.2f}%\"\n",
    "})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save results as CSV\n",
    "OUTPUT_PATH = os.path.join(DATASET_PATH, \"zero_shot_baseline_results.csv\")\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"Results saved to {OUTPUT_PATH}\")\n",
    "print(f\"Accuracy: {correct_predictions}/{total_samples} ({(correct_predictions/total_samples)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f0a15-f00c-4dc3-bd99-7574d8f79011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(OUTPUT_PATH)\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
