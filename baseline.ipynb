{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e534759b-3b65-46f5-b2ac-fc20a5cb03ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 0.26.0 not found\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch pandas tqdm accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2337c28e-5d6a-4cae-bd19-36c45a90c249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from TextToCypherDataLoader import Text2CypherDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Enable TensorFloat32 for faster matrix operations\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Paths\n",
    "DATASET_PATH = \"/work/pi_wenlongzhao_umass_edu/9/dpatel/\"\n",
    "MODEL_PATH = \"/datasets/ai/\"\n",
    "DEEPSEEK_DISTILL_LLAMA_70B_PATH = \"deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-70B/snapshots/0d6d11a6ea1187363aa7b78543f824fc02e06b14\"\n",
    "DEEPSEEK_DISTILL_QWEN_7B_PATH = \"deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B/snapshots/6602cadec947dbb53e64f3d8d6425320b2197247\"\n",
    "\n",
    "DEEPSEEK_DISTILL_LLAMA_70B = os.path.join(MODEL_PATH, DEEPSEEK_DISTILL_LLAMA_70B_PATH)\n",
    "DEEPSEEK_DISTILL_QWEN_7B = os.path.join(MODEL_PATH, DEEPSEEK_DISTILL_QWEN_7B_PATH)\n",
    "\n",
    "dataset = load_dataset(\"neo4j/text2cypher-2024v1\")[\"train\"].shuffle(seed=42).select(range(1))\n",
    "\n",
    "MODEL_LIST = [\n",
    "    (DEEPSEEK_DISTILL_QWEN_7B, \"DeepSeek Distill QWEN 7B\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1baa65f-bf72-4193-a1ef-aac75372dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d1ebf-5cbe-4203-abf3-4e8d1a8c3499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cypher(batch_questions, batch_schemas, model, tokenizer):\n",
    "    batch_inputs = [\n",
    "        (\n",
    "            \"### Instruction:\\n\"\n",
    "            \"Convert the given natural language question into a Cypher query.\\n\"\n",
    "            \"Do not explain your answer. Do not add any extra text.\\n\"\n",
    "            \"Just return the Cypher query.\\n\\n\"\n",
    "            \"### Input:\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            f\"Schema: {schema}\\n\"\n",
    "            \"### Output:\\n\"\n",
    "            \"Cypher Query:\"\n",
    "        ) for question, schema in zip(batch_questions, batch_schemas)\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer(batch_inputs, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    inputs = {key: value.to(\"cuda\") for key, value in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=256, \n",
    "            do_sample=False, \n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    return tokenizer.batch_decode(output_tokens.cpu(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcee445c-7a57-4661-8566-bb37ad263e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Cypher Queries:   0%|          | 0/2 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating Cypher Queries:  50%|█████     | 1/2 [22:39<22:39, 1359.23s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating Cypher Queries: 100%|██████████| 2/2 [27:41<00:00, 830.89s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /work/pi_wenlongzhao_umass_edu/9/dpatel/zero_shot_baseline_results.csv\n",
      "Accuracy: 0/10 (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_baseline(train_loader, model, tokenizer, name):\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Generating Cypher Queries - {name}\"):\n",
    "        batch_questions = batch[\"question\"]  # ✅ Now accessible\n",
    "        batch_schemas = batch[\"schema\"]  # ✅ Now accessible\n",
    "        batch_true_cyphers = batch[\"cypher\"]  # ✅ Now accessible\n",
    "\n",
    "        predicted_cyphers = generate_cypher(batch_questions, batch_schemas, model, tokenizer)\n",
    "\n",
    "        for true_cypher, predicted_cypher in zip(batch_true_cyphers, predicted_cyphers):\n",
    "            is_correct = predicted_cypher.strip().lower() == true_cypher.strip().lower()\n",
    "            if is_correct:\n",
    "                correct_predictions += 1\n",
    "            total_samples += 1\n",
    "\n",
    "    new_result = {\n",
    "        \"Model Name\": name,\n",
    "        \"Correctly Predicted\": correct_predictions,\n",
    "        \"Total Samples\": total_samples,\n",
    "        \"Accuracy\": f\"{(correct_predictions/total_samples)*100:.2f}%\"\n",
    "    }\n",
    "\n",
    "    OUTPUT_PATH = os.path.join(DATASET_PATH, \"zero_shot_baseline_results.csv\")\n",
    "    if os.path.exists(OUTPUT_PATH):\n",
    "        df = pd.read_csv(OUTPUT_PATH)\n",
    "        df = pd.concat([df, pd.DataFrame([new_result])], ignore_index=True)\n",
    "    else:\n",
    "        df = pd.DataFrame([new_result])\n",
    "    \n",
    "    df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "    print(f\"✅ Results saved to {OUTPUT_PATH} for model {name}\")\n",
    "    print(f\"✅ Accuracy: {correct_predictions}/{total_samples} ({(correct_predictions/total_samples)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_path, model_name in MODEL_LIST:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    train_dataset = Text2CypherDataset(dataset, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    generate_baseline(train_loader, model, tokenizer, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f0a15-f00c-4dc3-bd99-7574d8f79011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(OUTPUT_PATH)\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
